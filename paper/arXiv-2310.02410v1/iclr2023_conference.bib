@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{smith2022using,
  title={Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{hoffmann2022training,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@article{lepikhin2020gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}

@article{roller2021hash,
  title={Hash layers for large sparse models},
  author={Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17555--17566},
  year={2021}
}

@article{fedus2021switch,
  title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={arXiv preprint arXiv:2101.03961},
  year={2021}
}

@article{fedus2022review,
  title={A Review of Sparse Expert Models in Deep Learning},
  author={Fedus, William and Dean, Jeff and Zoph, Barret},
  journal={arXiv preprint arXiv:2209.01667},
  year={2022}
}

@article{zoph2022designing,
  title={Designing effective sparse expert models},
  author={Zoph, Barret and Bello, Irwan and Kumar, Sameer and Du, Nan and Huang, Yanping and Dean, Jeff and Shazeer, Noam and Fedus, William},
  journal={arXiv preprint arXiv:2202.08906},
  year={2022}
}

@article{rodriguez2018lower,
  title={Lower numerical precision deep learning inference and training},
  author={Rodriguez, Andres and Segal, Eden and Meiri, Etay and Fomenko, Evarist and Kim, Young Jin and Shen, Haihao and Ziv, Barukh},
  year={2018}
}

@inproceedings{kim2019research,
  title={From research to production and back: Ludicrously fast neural machine translation},
  author={Kim, Young Jin and Junczys-Dowmunt, Marcin and Hassan, Hany and Aji, Alham Fikri and Heafield, Kenneth and Grundkiewicz, Roman and Bogoychev, Nikolay},
  booktitle={Proceedings of the 3rd Workshop on Neural Generation and Translation},
  pages={280--288},
  year={2019}
}

@article{kim2020fastformers,
  title={Fastformers: Highly efficient transformer models for natural language understanding},
  author={Kim, Young Jin and Awadalla, Hany Hassan},
  journal={arXiv preprint arXiv:2010.13382},
  year={2020}
}

@article{kim2021scalable,
  title={Scalable and efficient moe training for multitask multilingual models},
  author={Kim, Young Jin and Awan, Ammar Ahmad and Muzio, Alexandre and Salinas, Andres Felipe Cruz and Lu, Liyang and Hendy, Amr and Rajbhandari, Samyam and He, Yuxiong and Awadalla, Hany Hassan},
  journal={arXiv preprint arXiv:2109.10465},
  year={2021}
}

@article{zuo2021taming,
  title={Taming sparsely activated transformer with stochastic experts},
  author={Zuo, Simiao and Liu, Xiaodong and Jiao, Jian and Kim, Young Jin and Hassan, Hany and Zhang, Ruofei and Zhao, Tuo and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2110.04260},
  year={2021}
}

@inproceedings{liu2022gating,
  title={Gating Dropout: Communication-efficient Regularization for Sparsely Activated Transformers},
  author={Liu, Rui and Kim, Young Jin and Muzio, Alexandre and Hassan, Hany},
  booktitle={International Conference on Machine Learning},
  pages={13782--13792},
  year={2022},
  organization={PMLR}
}

@article{stock2019and,
  title={And the bit goes down: Revisiting the quantization of neural networks},
  author={Stock, Pierre and Joulin, Armand and Gribonval, R{\'e}mi and Graham, Benjamin and J{\'e}gou, Herv{\'e}},
  journal={arXiv preprint arXiv:1907.05686},
  year={2019}
}

@article{Gholami2022ASO,
  title={A Survey of Quantization Methods for Efficient Neural Network Inference},
  author={Amir Gholami and Sehoon Kim and Zhen Dong and Zhewei Yao and Michael W. Mahoney and Kurt Keutzer},
  journal={ArXiv},
  year={2022},
  volume={abs/2103.13630}
}

@article{Choukroun2019LowbitQO,
  title={Low-bit Quantization of Neural Networks for Efficient Inference},
  author={Yoni Choukroun and Eli Kravchik and Pavel Kisilev},
  journal={2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},
  year={2019},
  pages={3009-3018}
}

@article{Fan2021TrainingWQ,
  title={Training with Quantization Noise for Extreme Model Compression},
  author={Angela Fan and Pierre Stock and Benjamin Graham and Edouard Grave and R{\'e}mi Gribonval and Herv{\'e} J{\'e}gou and Armand Joulin},
  journal={ArXiv},
  year={2021},
  volume={abs/2004.07320}
}

@article{Bai2021BinaryBERTPT,
  title={BinaryBERT: Pushing the Limit of BERT Quantization},
  author={Haoli Bai and Wei Zhang and Lu Hou and Lifeng Shang and Jing Jin and Xin Jiang and Qun Liu and Michael R. Lyu and Irwin King},
  journal={ArXiv},
  year={2021},
  volume={abs/2012.15701}
}

@article{Dettmers2022LLMint88M,
  title={LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author={Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer},
  journal={ArXiv},
  year={2022},
  volume={abs/2208.07339}
}

@article{Yao2022ZeroQuantEA,
  title={ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers},
  author={Zhewei Yao and Reza Yazdani Aminabadi and Minjia Zhang and Xiaoxia Wu and Conglong Li and Yuxiong He},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.01861}
}

@inproceedings{Aji2020CompressingNM,
  title={Compressing Neural Machine Translation Models with 4-bit Precision},
  author={Alham Fikri Aji and Kenneth Heafield},
  booktitle={NGT},
  year={2020}
}

@inproceedings{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle={NIPS},
  year={2017}
}

@article{Rasley2020DeepSpeedSO,
  title={DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters},
  author={Jeff Rasley and Samyam Rajbhandari and Olatunji Ruwase and Yuxiong He},
  journal={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  year={2020}
}

@inproceedings{Ren2021ZeROOffloadDB,
  title={ZeRO-Offload: Democratizing Billion-Scale Model Training},
  author={Jie Ren and Samyam Rajbhandari and Reza Yazdani Aminabadi and Olatunji Ruwase and Shuangyang Yang and Minjia Zhang and Dong Li and Yuxiong He},
  booktitle={USENIX Annual Technical Conference},
  year={2021}
}

@article{Shazeer2018MeshTensorFlowDL,
  title={Mesh-TensorFlow: Deep Learning for Supercomputers},
  author={Noam M. Shazeer and Youlong Cheng and Niki Parmar and Dustin Tran and Ashish Vaswani and Penporn Koanantakool and Peter Hawkins and HyoukJoong Lee and Mingsheng Hong and Cliff Young and Ryan Sepassi and Blake A. Hechtman},
  journal={ArXiv},
  year={2018},
  volume={abs/1811.02084}
}

@inproceedings{Lewis2021BASELS,
  title={BASE Layers: Simplifying Training of Large, Sparse Models},
  author={Mike Lewis and Shruti Bhosale and Tim Dettmers and Naman Goyal and Luke Zettlemoyer},
  booktitle={ICML},
  year={2021}
}

@inproceedings{Rajbhandari2022DeepSpeedMoEAM,
  title={DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale},
  author={Samyam Rajbhandari and Conglong Li and Zhewei Yao and Minjia Zhang and Reza Yazdani Aminabadi and Ammar Ahmad Awan and Jeff Rasley and Yuxiong He},
  booktitle={ICML},
  year={2022}
}

@inproceedings{Kudugunta2021BeyondDT,
  title={Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference},
  author={Sneha Kudugunta and Yanping Huang and Ankur Bapna and Maxim Krikun and Dmitry Lepikhin and Minh-Thang Luong and Orhan Firat},
  booktitle={EMNLP},
  year={2021}
}

@article{Zhou2022MixtureofExpertsWE,
  title={Mixture-of-Experts with Expert Choice Routing},
  author={Yan-Quan Zhou and Tao Lei and Han-Chu Liu and Nan Du and Yanping Huang and Vincent Zhao and Andrew M. Dai and Zhifeng Chen and Quoc V. Le and James Laudon},
  journal={ArXiv},
  year={2022},
  volume={abs/2202.09368}
}

@article{wenzek2019ccnet,
  title={Ccnet: Extracting high quality monolingual datasets from web crawl data},
  author={Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm{\'a}n, Francisco and Joulin, Armand and Grave, Edouard},
  journal={arXiv preprint arXiv:1911.00359},
  year={2019}
}

@inproceedings{Kasai2021DeepES,
  title={Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation},
  author={Jungo Kasai and Nikolaos Pappas and Hao Peng and James Cross and Noah A. Smith},
  booktitle={ICLR},
  year={2021}
}

@article{Ke2021RethinkingPE,
  title={Rethinking Positional Encoding in Language Pre-training},
  author={Guolin Ke and Di He and Tie-Yan Liu},
  journal={ArXiv},
  year={2021},
  volume={abs/2006.15595}
}

@inproceedings{Xiong2020OnLN,
  title={On Layer Normalization in the Transformer Architecture},
  author={Ruibin Xiong and Yunchang Yang and Di He and Kai Zheng and Shuxin Zheng and Chen Xing and Huishuai Zhang and Yanyan Lan and Liwei Wang and Tie-Yan Liu},
  booktitle={ICML},
  year={2020}
}

@article{hoefler2021sparsity,
  title={Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks.},
  author={Hoefler, Torsten and Alistarh, Dan and Ben-Nun, Tal and Dryden, Nikoli and Peste, Alexandra},
  journal={J. Mach. Learn. Res.},
  volume={22},
  number={241},
  pages={1--124},
  year={2021}
}

@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}

@inproceedings{elsen2020fast,
  title={Fast sparse convnets},
  author={Elsen, Erich and Dukhan, Marat and Gale, Trevor and Simonyan, Karen},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={14629--14638},
  year={2020}
}

@article{see2016compression,
  title={Compression of neural machine translation models via pruning},
  author={See, Abigail and Luong, Minh-Thang and Manning, Christopher D},
  journal={arXiv preprint arXiv:1606.09274},
  year={2016}
}

@inproceedings{behnke2020losing,
  title={Losing heads in the lottery: Pruning transformer attention in neural machine translation},
  author={Behnke, Maximiliana and Heafield, Kenneth},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={2664--2674},
  year={2020}
}

@inproceedings{gondimalla2019sparten,
  title={SparTen: A sparse tensor accelerator for convolutional neural networks},
  author={Gondimalla, Ashish and Chesnut, Noah and Thottethodi, Mithuna and Vijaykumar, TN},
  booktitle={Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={151--165},
  year={2019}
}

@article{parashar2017scnn,
  title={SCNN: An accelerator for compressed-sparse convolutional neural networks},
  author={Parashar, Angshuman and Rhu, Minsoo and Mukkara, Anurag and Puglielli, Antonio and Venkatesan, Rangharajan and Khailany, Brucek and Emer, Joel and Keckler, Stephen W and Dally, William J},
  journal={ACM SIGARCH computer architecture news},
  volume={45},
  number={2},
  pages={27--40},
  year={2017},
  publisher={ACM New York, NY, USA}
}

@article{Zhou2018CambriconSAI,
  title={Cambricon-S: Addressing Irregularity in Sparse Neural Networks through A Cooperative Software/Hardware Approach},
  author={Xuda Zhou and Zidong Du and Qi Guo and Shaoli Liu and Chengsi Liu and Chao Wang and Xuehai Zhou and Ling Li and Tianshi Chen and Yunji Chen},
  journal={2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  year={2018},
  pages={15-28}
}

@article{Zhu2019SparseTC,
  title={Sparse Tensor Core: Algorithm and Hardware Co-Design for Vector-wise Sparse Neural Networks on Modern GPUs},
  author={Maohua Zhu and Tao Zhang and Zhenyu Gu and Yuan Xie},
  journal={Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture},
  year={2019}
}

@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@article{sanh2020movement,
  title={Movement pruning: Adaptive sparsity by fine-tuning},
  author={Sanh, Victor and Wolf, Thomas and Rush, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20378--20389},
  year={2020}
}

@misc{baines2021fairscale,
  title={Fairscale: A general purpose modular pytorch library for high performance and large scale training},
  author={Baines, Mandeep and Bhosale, Shruti and Caggiano, Vittorio and Goyal, Naman and Goyal, Siddharth and Ott, Myle and Lefaudeux, Benjamin and Liptchinsky, Vitaliy and Rabbat, Mike and Sheiffer, Sam and others},
  year={2021}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer.},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J and others},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}
@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}
@inproceedings{clark2022unified,
  title={Unified scaling laws for routed language models},
  author={Clark, Aidan and de Las Casas, Diego and Guy, Aurelia and Mensch, Arthur and Paganini, Michela and Hoffmann, Jordan and Damoc, Bogdan and Hechtman, Blake and Cai, Trevor and Borgeaud, Sebastian and others},
  booktitle={International Conference on Machine Learning},
  pages={4057--4086},
  year={2022},
  organization={PMLR}
}
@article{artetxe2021efficient,
  title={Efficient large scale language modeling with mixtures of experts},
  author={Artetxe, Mikel and Bhosale, Shruti and Goyal, Naman and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Lin, Xi Victoria and Du, Jingfei and Iyer, Srinivasan and Pasunuru, Ramakanth and others},
  journal={arXiv preprint arXiv:2112.10684},
  year={2021}
}

@article{narayan2018don,
  title={Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={arXiv preprint arXiv:1808.08745},
  year={2018}
}

@article{wu2020integer,
  title={Integer quantization for deep learning inference: Principles and empirical evaluation},
  author={Wu, Hao and Judd, Patrick and Zhang, Xiaojie and Isaev, Mikhail and Micikevicius, Paulius},
  journal={arXiv preprint arXiv:2004.09602},
  year={2020}
}

@article{bengio2013estimating,
  title={Estimating or propagating gradients through stochastic neurons for conditional computation},
  author={Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  journal={arXiv preprint arXiv:1308.3432},
  year={2013}
}

@article{kim2016sequence,
  title={Sequence-level knowledge distillation},
  author={Kim, Yoon and Rush, Alexander M},
  journal={arXiv preprint arXiv:1606.07947},
  year={2016}
}

@article{wang2020multi,
  title={Multi-task learning for multilingual neural machine translation},
  author={Wang, Yiren and Zhai, ChengXiang and Awadalla, Hany Hassan},
  journal={arXiv preprint arXiv:2010.02523},
  year={2020}
}